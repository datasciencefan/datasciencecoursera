---
title: "PML Project"
author: "Thomas"
date: "Thursday, February 12, 2015"
output: html_document
---

1. Load the Data
First we load the main libraries
```{r Load Library, cache= TRUE, warning=FALSE}
library(caret)
library(kernlab)
```

Then we load the data
```{r Load Data, cache= TRUE, warning=FALSE}
train <- read.csv("C:/Users/Betty/Box Sync/Courses/Coursera - Practical Machine Learning/pml-training.csv")
test <- read.csv("C:/Users/Betty/Box Sync/Courses/Coursera - Practical Machine Learning/pml-testing.csv")

```

2. Preprocessing
Now we preprocess the data
```{r PreProcess, cache= TRUE, warning=FALSE}
# Removing summary rows/columns
tr <- subset(train, (train[ ,12] == "")) # remove summary rows
tr[tr == ""] <- "NA"
tr <- Filter(function(x)!all(is.na(x)), tr)  # remove empty columns

# Remove zero variable predictors
nsv <- nearZeroVar(tr, saveMetrics=TRUE) #check near zero variates
tr<- tr[,colnames(tr)!="new_window"] #remove the zero variate columns

#Remove other data columns for cleaning purposes
tr <- tr[,-which(names(tr) %in% c("user_name","cvtd_timestamp"))]

#To test out of sample error, we break the training set into two - trA and trB
inTrain <- createDataPartition(y=tr$classe, p=0.7, list=FALSE)
trA <- droplevels(tr[inTrain,])
trB <- droplevels(tr[-inTrain,])

#Create the test set with the same filtered columns as the training sets
ts <- test
ts <- as.data.frame(lapply(intersect(names(ts), names(tr)), 
              function(name) ts[name]))
```

3. Prediction
Now we apply a random forest prediction method.
The caret train function was not used since computing time was too long.
We use the randomForest function directly.
```{r Prediction, cache= TRUE, warning=FALSE}
library(randomForest)
library(ROCR)
#bestmtry <- tuneRF(tr[,-57],tr[,57], ntreeTry=100, 
#     stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE, dobest=FALSE) 
#We ran the above earlier and got mtry=10 as a result
exercise.rf <-randomForest(classe~.,data=trA, mtry=10, ntree=1000, 
     keep.forest=TRUE, importance=TRUE,test=trB) #create the random forest model
```

Review Importance of Variables
```{r Check Variables, cache= TRUE, warning=FALSE}
importance(exercise.rf)
varImpPlot(exercise.rf) #See how much each variable contributes to the model
```

Out of Sample Error Rate
This is calculated by looking at the accuracy of the prediction for the training set B.

```{r Out of Sample Error, cache= TRUE, warning=FALSE}
pred <- predict(exercise.rf, trB, OOB=TRUE) #predict using train set B as test
confusionMatrix(trB$classe, pred) #load confusion matrix
```

From the confusion matrix, we see that our model is in a sense, perfectly matches the actual values. This means our out of sample error is nearly 0.


Predict Using Test Set
```{r Predict with Test Set, cache= TRUE, warning=FALSE}
pred <- predict(exercise.rf, ts, OOB=TRUE)
pred
```
